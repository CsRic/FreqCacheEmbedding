{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward time comparison: 0.0004940032958984375s : 0.00017881393432617188s: 0.00563812255859375s\n",
      "Backward time comparison: 0.0012764930725097656s : 0.0002722740173339844s: 2.193450927734375e-05s\n",
      "memory comparison: blk_embed:512mb\n",
      "                        : costly_embed:2048mb\n",
      "                        : qr_embed:4mb\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.profiler import profile, ProfilerActivity, tensorboard_trace_handler\n",
    "from recsys.modules.embeddings import BlockEmbeddingBag, QREmbeddingBag\n",
    "\n",
    "# Example inputs\n",
    "vocab_size = 1024*1024 # |V|\n",
    "embedding_dim = 512 # E_e\n",
    "block_embedding_dim = 128 # E_b\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_shape = (16384, 16384)\n",
    "input_x = torch.randint(0, vocab_size, input_shape).to(device)\n",
    "reduce_op = 'max'\n",
    "\n",
    "# Initiate modules\n",
    "blk_embed = BlockEmbeddingBag(\n",
    "                vocab_size, \n",
    "                block_embedding_dim,\n",
    "                embedding_dim,\n",
    "                mode=reduce_op,\n",
    "                device=device)\n",
    "\n",
    "qr_embed = QREmbeddingBag(\n",
    "                embedding_dim,\n",
    "                num_buckets=math.ceil(math.sqrt(vocab_size)),\n",
    "                verbose=False).to(device)\n",
    "\n",
    "costly_embed = nn.EmbeddingBag(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            mode=reduce_op,\n",
    "            device=device)\n",
    "\n",
    "# Query step:\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "blk_output = blk_embed(input_x.clone())\n",
    "t2 = time.time()\n",
    "costly_output = costly_embed(input_x.clone())\n",
    "t3 = time.time()\n",
    "# with profile(\n",
    "#         activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "#         profile_memory=True, \n",
    "#         record_shapes=True,\n",
    "#         on_trace_ready=tensorboard_trace_handler('prof-log/qr_emb_fw'),\n",
    "# ) as prof:\n",
    "qr_output = qr_embed(input_x.clone())\n",
    "t4 = time.time()\n",
    "\n",
    "grad = torch.randn(blk_output.shape).to(device)\n",
    "t5 = time.time()\n",
    "blk_output.backward(grad.clone())\n",
    "t6 = time.time()\n",
    "costly_output.backward(grad.clone())\n",
    "t7 = time.time()\n",
    "# qr_output.backward(grad.clone())\n",
    "t8 = time.time()\n",
    "\n",
    "print(f'Forward time comparison: {t2-t1}s : {t3-t2}s: {t4-t3}s')\n",
    "print(f'Backward time comparison: {t6-t5}s : {t7-t6}s: {t8-t7}s')\n",
    "\n",
    "def compute_mem(model): \n",
    "    mem_params = sum([param.nelement()*param.element_size() \n",
    "                    for param in model.parameters()])\n",
    "    mem_bufs = sum([buf.nelement()*buf.element_size() \n",
    "                    for buf in model.buffers()])\n",
    "    mem = mem_params + mem_bufs\n",
    "    return mem // 1024**2\n",
    "\n",
    "print(f'''memory comparison: blk_embed:{compute_mem(blk_embed)}mb\n",
    "                        : costly_embed:{compute_mem(costly_embed)}mb\n",
    "                        : qr_embed:{compute_mem(qr_embed)}mb''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For costly embed (w/ 4dp), the maximum supported embedding size is 35 * 0.5b = 17.5b\n",
    "For block (mv) embed (w/ 4tp+dp), the maximum supported embedding size is 35 * 0.5b * 16 = 280b\n",
    "\n",
    "Save exponentially small memory to the world size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "world size is w\n",
    "vocabulary size is V\n",
    "embedding dim is E\n",
    "block embedding dim is E//w\n",
    "\n",
    "total saved space from all devices = (V//w*E - V//w*E//w - E//w*E) * w\n",
    "\n",
    "#param limit at one device = l\n",
    "new #param limit = l * w**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communication: allreduce step \n",
    "    - Round-robin: O((w-1)*V*E)\n",
    "Computation: linear layer:\n",
    "    - O(w*E*E//w)\n",
    "Other cost: embedding lookup:\n",
    "    - O(w*V//w)\n",
    "\n",
    "Overall, time complexity is O(V+V*E*(w-1)+E^2) = O(V) when V >> E >> w, which is identical to one single embedding (V,E)'s lookup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottleneck of qr embedding is two embedding lookups, which can be parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache size= cache_sets * cache_lines * embedding_dim * element_size\n",
    "\n",
    "cache_sets = 500_000\n",
    "cache_lines = 1\n",
    "embedding_dim = 256\n",
    "element_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "model = torch.nn.Linear(10,10)\n",
    "[param.element_size() for param in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.476837158203125"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_sets = 500_000\n",
    "cache_lines = 1\n",
    "embedding_dim = 256\n",
    "element_size = 4\n",
    "\n",
    "cache_size= cache_sets * cache_lines * embedding_dim * element_size\n",
    "cache_size / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Passed.\n",
    "def lbmgr_fair_divide(field_dims,num_groups) -> None:\n",
    "    dim_per_rank = sum(field_dims) // num_groups\n",
    "    dim_indices = np.array(range(len(field_dims)))\n",
    "    \n",
    "    cuts = dict()\n",
    "    num_cuts = 0\n",
    "    \n",
    "    agg = dim_per_rank\n",
    "    # Find cut positions and shard groups\n",
    "    for ind in dim_indices:\n",
    "        while field_dims[ind] > agg:\n",
    "            if num_cuts >= num_groups - 1:\n",
    "                break\n",
    "            if ind in cuts:\n",
    "                cuts[ind].append(agg)\n",
    "            else:\n",
    "                cuts[ind] = [agg]\n",
    "            agg += dim_per_rank\n",
    "            num_cuts += 1\n",
    "        \n",
    "        agg -= field_dims[ind]\n",
    "    \n",
    "    emb_dim = dim_per_rank\n",
    "    \n",
    "    return cuts, emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts, emb_dim = lbmgr_fair_divide([10,200,400,2000,1000,500],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensor = torch.randn((12,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 2.2175, 1.1030, 0.3941, 0.6608, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 2.0703, 0.0000, 1.5998],\n",
       "        [0.0000, 1.5906, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.8165, 0.0000, 0.0000, 0.7085],\n",
       "        [0.0000, 0.0000, 0.1005, 0.0087, 1.3287, 0.7515],\n",
       "        [0.0000, 0.5046, 0.0000, 0.0000, 0.0000, 0.4024],\n",
       "        [0.0000, 0.0000, 0.0000, 1.4834, 0.0000, 0.0000],\n",
       "        [0.0000, 0.1852, 0.0000, 2.3544, 0.4991, 0.3674],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2637, 0.5050, 2.1840, 0.0000, 0.0000, 0.7402],\n",
       "        [0.1266, 0.0000, 0.4667, 0.0000, 0.3489, 0.0000],\n",
       "        [1.0668, 1.1004, 0.3868, 0.7628, 1.3871, 0.9825]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(tensor,torch.zeros(tensor.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5253,\n",
       " {2: [3133], 4: [1749], 5: [3979]},\n",
       " [[0, 1, 2], [2, 3, 4], [4, 5], [5, 6, 7]],\n",
       " [tensor([   0,  100, 2120]),\n",
       "  tensor([   0,   70, 3504]),\n",
       "  tensor([   0, 1274]),\n",
       "  tensor([  0, 566, 689])])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "embeddings_per_feat=[100,2020,3203,3434,3023,4545,123,4566]\n",
    "num_groups = 4\n",
    "\n",
    "def fair_initialize(embeddings_per_feat=embeddings_per_feat,num_groups=num_groups) -> None:\n",
    "    num_embeddings_per_rank = sum(embeddings_per_feat) // num_groups\n",
    "    dim_indices = np.array(range(len(embeddings_per_feat)))\n",
    "    groups = []\n",
    "    offsets = []\n",
    "    _curr_grp = []\n",
    "    _curr_offs = [0]\n",
    "\n",
    "    cuts = dict()\n",
    "    _num_cuts = 0\n",
    "    _agg = num_embeddings_per_rank\n",
    "    # Find cut positions and shard groups\n",
    "    for ind in dim_indices:\n",
    "        while embeddings_per_feat[ind] > _agg:\n",
    "            if _num_cuts >= num_groups - 1: # never cut when enough groups\n",
    "                break\n",
    "            if ind in cuts.keys():\n",
    "                cuts[ind].append(_agg)\n",
    "            else:\n",
    "                cuts[ind] = [_agg]\n",
    "            _num_cuts += 1\n",
    "            \n",
    "            offsets.append(torch.tensor(_curr_offs))\n",
    "            _curr_offs = [0]\n",
    "            _curr_grp.append(ind)\n",
    "            groups.append(_curr_grp)\n",
    "            _curr_grp = []\n",
    "            \n",
    "            _agg += num_embeddings_per_rank\n",
    "        \n",
    "        if _agg >= embeddings_per_feat[ind] and len(_curr_offs) == 1:\n",
    "            _curr_offs.append(embeddings_per_feat[ind]-(_agg-num_embeddings_per_rank))\n",
    "        else:\n",
    "            _curr_offs.append(embeddings_per_feat[ind])\n",
    "        \n",
    "        _agg -= embeddings_per_feat[ind]\n",
    "        _curr_grp.append(ind)\n",
    "    \n",
    "    offsets.append(torch.tensor(_curr_offs[:-1]))\n",
    "    for i in range(len(offsets)):\n",
    "        offsets[i] = torch.cumsum(offsets[i], dim=0)\n",
    "    groups.append(_curr_grp)\n",
    "        \n",
    "    return num_embeddings_per_rank, cuts, groups, offsets\n",
    "\n",
    "fair_initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete load balancer\n",
    "from recsys.modules.embeddings import LoadBalanceManager\n",
    "    \n",
    "lbmgr = LoadBalanceManager([100,2020,3203,3434302,3023,45459,123,4566],4,128,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_input = []\n",
    "\n",
    "for i in range(len([100,2020,3203,3434302,3023,45459,123,4566])):\n",
    "    rand_input.append(torch.randint(0,[100,2020,3203,3434302,3023,45459,123,4566][i],\n",
    "                                            size=(8,)).unsqueeze(1))\n",
    "    \n",
    "rand_input_tensor = torch.cat(rand_input,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     87,     573,     812, 2086251,    2501,   10109,     108,    2057],\n",
       "        [      1,      29,    1133, 3358788,    1908,    1268,      15,    2393],\n",
       "        [     10,    1185,    1839, 2375642,    1997,   20508,      17,    4385],\n",
       "        [     93,     586,     916, 2344477,    1541,   19701,      84,    4052],\n",
       "        [     54,    1904,    1948, 1378270,     570,   25398,      32,    1569],\n",
       "        [     16,    1674,    1889,  471842,    1077,   38336,     117,    2978],\n",
       "        [     60,    1044,    1953, 3246596,     625,   19882,      20,    2391],\n",
       "        [     69,    1381,    1512, 1169965,     442,   29359,      71,    1383]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 2020, 3203, 3434302, 3023, 45459, 123, 4566]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbmgr.embeddings_per_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([      0, 3434302]),\n",
       " tensor([   0, 2020]),\n",
       " tensor([    0, 45459]),\n",
       " tensor([   0, 4566])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbmgr.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3, 0]), array([1, 2]), array([5, 6]), array([7, 4])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbmgr.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3434402"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbmgr.get_num_embeddings_on_rank(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     69,    1590,    2101, 3002450,    1443,   18711,       3,    4532],\n",
      "        [     83,    1153,    2446, 1065773,     561,   31027,      58,    3101],\n",
      "        [     40,     396,     967, 1962002,    1553,   24817,      54,      77],\n",
      "        [     38,     888,     843, 2855171,    1787,   22663,     107,     395],\n",
      "        [     42,    1190,    2661, 1749865,    2260,    2255,      96,    2929],\n",
      "        [     76,     821,    1003, 1770283,    2643,   13591,     118,    2505],\n",
      "        [     58,    1583,     151, 1194557,     109,   35343,     109,    3112],\n",
      "        [     36,    1546,    2257, 2188749,     462,   29215,      51,     226]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3002450, 3434371],\n",
       "        [1065773, 3434385],\n",
       "        [1962002, 3434342],\n",
       "        [2855171, 3434340],\n",
       "        [1749865, 3434344],\n",
       "        [1770283, 3434378],\n",
       "        [1194557, 3434360],\n",
       "        [2188749, 3434338]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rand_input_tensor)\n",
    "lbmgr.shard_tensor(rand_input_tensor,rank=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('convenv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8fa1a750837aad8f68226b49ddff2eea531a9aec85219b5a5f98ebb47fccd095"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
